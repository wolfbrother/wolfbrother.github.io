<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-flash.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico?v=5.1.4">


  <link rel="mask-icon" href="/favicon.ico?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习,">





  <link rel="alternate" href="/atom.xml" title="狼哥空间" type="application/atom+xml">






<meta name="description" content="第二章感知机 2.3.1 感知机学习算法的原始形式  感知机的学习算法是错误的分类样本锁驱动的，采用随机梯度下降法。 当没有错误分类时，学习停止。因此可知对于线性可分数据集，感知机的收敛解有无数个。  第三章 k近邻法 3.2.3 k值的选择  k值减小意味着模型变得复杂，容易过拟合 k值增大意味着模型变得简单，例如\(k=N\)时最简单。  3.2.4 分类决策规则">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="统计学习方法的总结">
<meta property="og:url" content="https://wolfbrother.github.io/2019/09/09/ML-AI/2019-09-09-统计学习方法的总结/index.html">
<meta property="og:site_name" content="狼哥空间">
<meta property="og:description" content="第二章感知机 2.3.1 感知机学习算法的原始形式  感知机的学习算法是错误的分类样本锁驱动的，采用随机梯度下降法。 当没有错误分类时，学习停止。因此可知对于线性可分数据集，感知机的收敛解有无数个。  第三章 k近邻法 3.2.3 k值的选择  k值减小意味着模型变得复杂，容易过拟合 k值增大意味着模型变得简单，例如\(k=N\)时最简单。  3.2.4 分类决策规则">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2020-03-20T06:11:23.157Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="统计学习方法的总结">
<meta name="twitter:description" content="第二章感知机 2.3.1 感知机学习算法的原始形式  感知机的学习算法是错误的分类样本锁驱动的，采用随机梯度下降法。 当没有错误分类时，学习停止。因此可知对于线性可分数据集，感知机的收敛解有无数个。  第三章 k近邻法 3.2.3 k值的选择  k值减小意味着模型变得复杂，容易过拟合 k值增大意味着模型变得简单，例如\(k=N\)时最简单。  3.2.4 分类决策规则">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: 'undefined',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://wolfbrother.github.io/2019/09/09/ML-AI/2019-09-09-统计学习方法的总结/">





  <title>统计学习方法的总结 | 狼哥空间</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">狼哥空间</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">虚怀若谷，求知若渴</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wolfbrother.github.io/2019/09/09/ML-AI/2019-09-09-统计学习方法的总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wolfbrother">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="狼哥空间">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">统计学习方法的总结</h2>
        

        <div class="post-meta">
          

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-09T00:00:00+08:00">
                2019-09-09
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML-AI/" itemprop="url" rel="index">
                    <span itemprop="name">ML-AI</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/09/09/ML-AI/2019-09-09-统计学习方法的总结/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/09/09/ML-AI/2019-09-09-统计学习方法的总结/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="第二章感知机">第二章感知机</h1>
<h3 id="感知机学习算法的原始形式">2.3.1 感知机学习算法的原始形式</h3>
<ul>
<li>感知机的学习算法是错误的分类样本锁驱动的，采用随机梯度下降法。</li>
<li>当没有错误分类时，学习停止。因此可知对于线性可分数据集，感知机的收敛解有无数个。</li>
</ul>
<h1 id="第三章-k近邻法">第三章 k近邻法</h1>
<h3 id="k值的选择">3.2.3 k值的选择</h3>
<ul>
<li>k值减小意味着模型变得复杂，容易过拟合</li>
<li>k值增大意味着模型变得简单，例如<span class="math inline">\(k=N\)</span>时最简单。</li>
</ul>
<h3 id="分类决策规则">3.2.4 分类决策规则</h3>
<ul>
<li>所用的多数表决规则，等价于0-1损失函数时的经验风险最小化。</li>
</ul>
<h1 id="第四章-朴素贝叶斯法">第四章 朴素贝叶斯法</h1>
<h2 id="朴素贝叶斯的学习与分类">4.1 朴素贝叶斯的学习与分类</h2>
<p>条件概率分布<span class="math inline">\(P(X\mid Y)\)</span>的参数个数是<span class="math inline">\(K\prod_{j=1}^nS_j\)</span>，是指数级的。先验概率<span class="math inline">\(P(Y)\)</span>的参数个数是<span class="math inline">\(K\)</span>。</p>
<h3 id="后验概率最大化的含义">4.1.2 后验概率最大化的含义</h3>
<p>容易验证朴素贝叶斯模型里，根据0-1损失函数时的期望风险最小化准则，可以推导出后验概率最大化准则。</p>
<ul>
<li>损失函数和期望风险函数参考公式1.5-1.9.</li>
</ul>
<h2 id="朴素贝叶斯的参数估计">4.2 朴素贝叶斯的参数估计</h2>
<p>可知朴素贝叶斯的先验概率<span class="math inline">\(p(y)\)</span>和似然函数<span class="math inline">\(p(x\mid y)\)</span>（也就是条件概率）是通过极大似然估计来计算得到的，然后通过后验概率最大来获得预测值p(yx)。</p>
<p>以公式4.8所示的先验概率<span class="math inline">\(P(Y=c_k)\)</span>的计算来说，用<span class="math inline">\(x\)</span>来指代该变量，然后用符号<span class="math inline">\(p\)</span>来指代观测到的<span class="math inline">\(y=c_k\)</span>的频率即<span class="math inline">\(\frac{\sum_{i=1}^NI(y_i=c_k)}{N}\)</span>，则<span class="math inline">\(x\)</span> 的似然函数为<span class="math inline">\(x^{pN}\cdot (1-x)^{(1-p)N}\)</span>，可以很容易计算该似然函数最大时<span class="math inline">\(x\)</span>取值与<span class="math inline">\(p\)</span>相等。同理可用同样的极大似然估计方法来计算似然函数<span class="math inline">\(p(x\mid y)\)</span>。</p>
<p>因此，第12章(p211)对朴素贝叶斯锁总结的学习策略有两个，即极大似然估计和极大后验概率估计。</p>
<h3 id="贝叶斯估计">4.2.3 贝叶斯估计</h3>
<p>样本不足时，上述极大似然估计得到的先验概率和条件概率时可能为0，显然是不合适的。</p>
<p>贝叶斯估计的处理办法：如果值有s种，则对每个概率的分子添加<span class="math inline">\(\lambda\)</span>值，对分母添加<span class="math inline">\(s\lambda\)</span>值。当<span class="math inline">\(\lambda\)</span>为0时就蜕化成了极大后验概率估计，当<span class="math inline">\(\lambda\)</span>为1时又叫做拉普拉斯平滑。</p>
<p>上述处理公式的推到，需要了解<a href="https://github.com/wolfbrother/biji-logs/blob/master/Algorithms/MachineLearning/20190527.md" target="_blank" rel="noopener">共轭先验</a>。</p>
<h1 id="第五章-决策树">第五章 决策树</h1>
<h2 id="决策树模型与学习">5.1 决策树模型与学习</h2>
<ul>
<li>决策树路径的特征：互斥并且完备。</li>
<li>每个叶节点上的条件概率偏向于某一类，分类时会将该节点的实例强制分到该类里。与朴素贝叶斯、最大熵模型不同的是，决策树并不能给出每个样本属于某个类的条件概率。</li>
<li>决策树学习算法包含三个过程：特征选择，决策树生成，剪枝。
<ul>
<li>决策树越深，模型越复杂。决策树生成只考虑局部最优，而剪枝考虑全局最优</li>
</ul></li>
</ul>
<h2 id="基于信息增益的特征选择">5.2 基于信息增益的特征选择</h2>
<ul>
<li>信息增益：特征A对训练数据集D的信息增益<span class="math inline">\(g(D,A)\)</span>，定义为集合D的类别信息的经验熵<span class="math inline">\(H(D)\)</span>与特征<span class="math inline">\(A\)</span>给定条件下D的经验条件熵<span class="math inline">\(H(D\mid A)\)</span>之差，即<span class="math inline">\(g(D,A)=H(D)-H(D\mid A)\)</span>。
<ul>
<li>之所以叫做经验熵和经验条件熵，是因为二者都是直接从数据集中根据极大似然估计所得到的（p61）。</li>
<li>信息增益<span class="math inline">\(g(D,A)\)</span>实际上是训练数据集中类与特征的互信息。</li>
<li>信息增益可以直观地看成给定特征<span class="math inline">\(A\)</span>之后，类别信息的不确定性的减少量，即特征<span class="math inline">\(A\)</span>的分类能力(公式5.7,5.8)。</li>
</ul></li>
<li>信息增益比
<ul>
<li>信息增益有个缺陷，如果某个特征的取值较多，其信息增益偏向于比较高。因此用该特征的经验熵<span class="math inline">\(H(A)\)</span>对其信息增益<span class="math inline">\(g(D,A)\)</span>进行归一化得到信息增益比<span class="math inline">\(g_r(D,A)\)</span>用于特征选择 （公式5.10）。</li>
</ul></li>
</ul>
<h2 id="决策树的生成">5.3 决策树的生成</h2>
<ul>
<li>有两种生成算法分别是ID3和C4.5，区别在于前者采用信息增益，后者采用信息增益比进行特征选择。</li>
<li>几个要点：
<ul>
<li>每条路径上特征最多只用一次。实际操作中，选择某个特征之后，将该特征从其子路径的特征集里剔除。</li>
<li>有个预设的信息增益阈值，如果所选特征的信息增益小于该阈值，则该路径停止，当前节点作为叶子节点。</li>
<li>路径停止的条件还有一个，即特征集为空的时候。</li>
</ul></li>
</ul>
<h2 id="决策树的剪枝">5.4 决策树的剪枝</h2>
<ul>
<li>减小模型复杂度，而模型复杂度用叶节点的个数来表示。</li>
<li>损失函数（公式5.11）包含两部分，一个是叶节点的类别信息的经验熵的加权求和（权重是该叶节点的样本数），另一个是模型复杂度。前者为0，则说明没有错误分类，其值越大，表示分类误差越大。后者越大，模型复杂度越大。因此剪枝过程对损失函数求极小。</li>
<li>剪枝时可以只计算局部的损失函数变化，因此：1.不用考虑叶节点的顺序；2.可以用动态规划来层层来剪枝。</li>
</ul>
<h2 id="cart算法">5.5 CART算法</h2>
<h3 id="cart生成">5.5.1 CART生成</h3>
<ul>
<li>回归树
<ul>
<li>一个回归树对应着特征空间的一个划分，以及在划分的单元上的一个常数的输出值。</li>
<li>适用于分段常数数据集</li>
<li>损失函数是平方误差和：1）给定某个划分单元，该单元的输出值将设定为均值；2）而最优划分点可以通过依次计算获得。有最小二乘法的影子，因此回归树又叫做最小二乘回归树。</li>
</ul></li>
<li>分类树
<ul>
<li>只是将决策树中的熵替换为基尼系数。</li>
<li>基尼系数、熵之半的曲线接近，可以近似分类误差率（图5.7）</li>
</ul></li>
</ul>
<h1 id="第六章-逻辑斯蒂回归于最大熵模型">第六章 逻辑斯蒂回归于最大熵模型</h1>
<p>最大熵模型的模型表达式，有介绍其推导过程。而逻辑斯蒂回归模型没有推导过程，而直接给出模型表达式。然后根据模型表达式，去优化参数，优化方法可以是梯度下降法和拟牛顿法，对于前者还有个专用的&quot;改进的迭代尺度法IIS&quot;。</p>
<h2 id="逻辑斯蒂回归模型">逻辑斯蒂回归模型</h2>
<ul>
<li>logistic函数也就是经常说的sigmoid函数： <span class="math display">\[\frac{1}{1+e^{-x}}\]</span> logistic函数在统计学和机器学习领域应用最为广泛或者最为人熟知的肯定是逻辑回归模型了。逻辑回归（Logistic Regression，简称LR）作为一种对数线性模型（log-linear model）被广泛地应用于分类和回归场景中。此外，logistic函数也是神经网络最为常用的激活函数，即sigmoid函数。</li>
</ul>
<h2 id="最大熵模型">6.2 最大熵模型</h2>
<h3 id="最大熵原理">6.2.1 最大熵原理</h3>
<p>最大熵原理认为要选择的概率模型首先必须满足已有的约束条件。而其余不确定的部分应该是等可能的。而等可能不容易操作，但由此而来的熵是一个可优化的数值指标。因此最大熵原理通过熵的最大化来表示等可能性。</p>
<h3 id="最大熵模型的学习">6.2.3 最大熵模型的学习</h3>
<p>对偶问题，即公式6.19的内层是关于条件概率<span class="math inline">\(p(y\mid x)\)</span>的函数，外层才是关于<span class="math inline">\(w\)</span>的函数。因此p84底部，求解内层时要对<span class="math inline">\(p(y\mid x)\)</span>求导。</p>
<p>求导得0，进而获得了最大熵模型（公式6.22）。</p>
<p>然后带入最大熵模型，求解外层（其表达式是公式6.27）。</p>
<h3 id="极大似然估计">6.2.4 极大似然估计</h3>
<p>对偶问题（公式6.19）的外层（公式6.27）是关于参数<span class="math inline">\(w\)</span>求解极大值的函数，而对数似然函数最大化（公式6.26）也是关于参数<span class="math inline">\(w\)</span>的求解极大值的函数。可以验证二者的公式，在最大熵模型时，是等价的。</p>
<h1 id="第七章-支持向量机">第七章 支持向量机</h1>
<h2 id="线性可分支持向量机与硬间隔最大化">7.1 线性可分支持向量机与硬间隔最大化</h2>
<ul>
<li>线性可分支持向量机的最优化问题 <span class="math display">\[min_{w,b}\frac{1}{2}\mid\mid w\mid\mid ^2, s.t. y_i(w\cdot x_i+b)-1&gt;=0\]</span></li>
</ul>
<p>其直观地推导方法如下：参考图7.3，假设<span class="math inline">\((w,b)\)</span>就是要找的超平面的参数，那么对于两侧的支持向量有<span class="math inline">\(y_i(w\cdot x_i+b)-1=0\)</span>，易知外侧的样本点<span class="math inline">\((x_i,y_i)\)</span>必有<span class="math inline">\(y_i(w\cdot x_i+b)-1&gt;0\)</span>，因此参数满足<span class="math inline">\(y_i(w\cdot x_i+b)-1&gt;=0\)</span>。根据平行面的距离公式，易知两个超平面的距离是<span class="math inline">\(\frac{2}{\mid\mid w\mid\mid }\)</span>，因此间隔最大化意味着$w$最小化。</p>
<p>因此支持向量机寻找超平面实际上就是寻找两个让支持向量集合里正例和反例分别是为<span class="math inline">\(+1/-1\)</span>的两个平行面，并且要使得这两个平行面的间隔尽可能地大。</p>
<ul>
<li>凸二次规划问题 <span class="math display">\[min_x f(x), s.t. g_i(x)&lt;=0,h_j(x)=0\]</span> 其中<span class="math inline">\(f(x)\)</span>和<span class="math inline">\(g(x)\)</span>是连续可微的二次凸函数，<span class="math inline">\(h(x)\)</span>是仿射函数。</li>
</ul>
<p>易知线性可分支持向量机的最优化问题，本质上是凸二次优化问题。</p>
<h3 id="学习的对偶算法">7.1.4 学习的对偶算法</h3>
<p>这一节很好地诠释了拉格朗日对偶性（附录 C）便于求解的作用： + 原始问题（公式7.13和7.14）引入拉格朗日算子（本质上是新的变量）去掉了约束条件，同时转化成了极大极小问题。该极大极小问题的内层的梯度为0的性质可以用来获得新的约束条件（公式7.20， 只含有拉格朗日算子，去掉了最初的参数<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>），将极大极小问题转换成一个有约束的极大问题（公式7.21），后者又叫做原始问题的对偶问题： <span class="math display">\[min_\alpha \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^{N}\alpha_i\]</span> <span class="math display">\[s.t. \sum_{i=1}^{N}\alpha_iy_i=0, \alpha_i\ge 0, i=1,2, \cdots, N\]</span></p>
<ul>
<li>拉格朗日对偶算法求解SVM的意义
<ul>
<li>降低求解复杂度：原问题的优化是一个二次规划问题，求解较麻烦，用拉格朗日乘子法转换后可以用smo等算法更简单地优化。</li>
<li>方便引入核函数：由于转换后的假设函数主要由内积运算构成，可以使用核函数简化特征映射到高维空间后的内积运算，高效地求解非线性问题。</li>
</ul></li>
</ul>
<h2 id="线性支持向量机与软间隔最大化">7.2 线性支持向量机与软间隔最大化</h2>
<p>注：p109 <span class="math inline">\(w\)</span>的解唯一，但<span class="math inline">\(b\)</span>的解不唯一存在一个区间。</p>
<p>近似线性可分时，给每个样本<span class="math inline">\((x_i,y_i)\)</span>引进一个松弛变量<span class="math inline">\(\xi_i\ge 0\)</span>，是的实际的函数间隔可以小于1甚至为负；并将松弛变量作为代价引入到原目标函数以对松弛变量进行限制，变成如下的原始问题： <span class="math display">\[min_{w,b,\xi}\frac{1}{2}\mid\mid w\mid\mid ^2+C\sum_{i=1}^{N}\xi_i\]</span> <span class="math display">\[s.t. y_i(w\cdot x_i+b)\ge 1-\xi_i, \xi_i\ge 0\]</span></p>
<p>其对偶问题是： <span class="math display">\[min_\alpha \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^{N}\alpha_i\]</span> <span class="math display">\[s.t. \sum_{i=1}^{N}\alpha_iy_i=0, 0\le\alpha_i\le C, i=1,2, \cdots, N\]</span></p>
<h3 id="支持向量">7.2.3 支持向量</h3>
<p>结合图7.5分析变量与空间位置关系：</p>
<ul>
<li>超平面<span class="math inline">\(w\cdot x +b =0\)</span>将<span class="math inline">\(w=\sum_{i=1}^{N}\alpha_iy_ix_i\)</span>带入进去，变成了<span class="math inline">\(\sum_{i=1}^{N}\alpha_iy_i(x\cdot x_i)+b=0\)</span>，因此只有<span class="math inline">\(\alpha_i&gt;0\)</span>的样本点为支持向量。</li>
<li>判别函数是<span class="math inline">\(f(x)=sign(\sum_{i=1}^{N}\alpha_iy_i(x\cdot x_i)+b)\)</span></li>
<li>由原始问题关于<span class="math inline">\(\xi\)</span>的含义，可知<span class="math inline">\(\xi=0\)</span>的样本点在间隔边界上或外面，<span class="math inline">\(0&lt;\xi&lt;1\)</span>的说明<span class="math inline">\(y_i(w\cdot x_i+b)=1-\xi\)</span>所以在间隔边界和超平面之间，<span class="math inline">\(\xi_i&gt;1\)</span>说明<span class="math inline">\(y_i(w\cdot x_i+b)=1-\xi&lt;0\)</span>位于超平面误分的一侧。</li>
</ul>
<h3 id="合页损失函数">合页损失函数</h3>
<p>上面7.2节的原始问题，等价于最优化问题： <span class="math display">\[\min_{w,b,\xi}\sum_{i=1}^{N}[1-y_i(w\cdot x_i+b)]_{+}+\lambda\mid\mid w\mid\mid ^2\]</span></p>
<p>其中前项是合页损失函数。</p>
<p>根据图7.6将合页损失函数与0-1损失和感知机的损失函数作对比，说明合页损失函数不仅要分类正确而且确信度足够高时损失才是0，即对学习有更高要求。</p>
<h2 id="非线性支持向量机与核函数">7.3 非线性支持向量机与核函数</h2>
<ul>
<li>在学习中只定义核函数<span class="math inline">\(K(x,y)\)</span>，而不显示地定义从输入空间到特征空间的映射函数<span class="math inline">\(\phi(x)\)</span>，二者关系：<span class="math inline">\(K(x,y)=(\phi(x)\cdot\phi(y))\)</span></li>
<li>对偶问题的目标函数的内积<span class="math inline">\((x_i\cdot x_j)\)</span>可以用核函数<span class="math inline">\(K(x_i,x_j)\)</span>来代替。</li>
</ul>
<h2 id="序列最小最优化算法smo">7.4 序列最小最优化算法SMO</h2>
<ul>
<li>SMO是一种启发式算法，其基本思路：
<ul>
<li>如果所有变量<span class="math inline">\(\alpha\)</span>都满足KKT条件，由于KKT条件是最优解的充分必要条件，则说明已经找到最优解。</li>
<li>不然以一定规则选择其中两个变量，固定其它变量，将问题转化成一个二元二次规划的子问题，对该子问题的优化会使得原目标函数值更小，而且该问题有解析解。</li>
<li>该两个变量的选取规则：一个是违反KKT条件（公式7.111-7.113）最严重的哪一个，另一个由约束条件自动确定。</li>
</ul></li>
</ul>
<h1 id="第八章-提升方法">第八章 提升方法</h1>
<h3 id="提升方法的基本思路">8.1.1 提升方法的基本思路</h3>
<p>AdaBoost如何实施提升方法的两个步骤：</p>
<ul>
<li>每一轮如何改变训练数据的权值或概率分布 提高哪些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。</li>
<li>如何将弱分类器组合成一个强分类器 加权线性组合的方式。具体的，分类误差率小的弱分类器的权值较大，而分类误差率大的弱分类器的权值较小。</li>
</ul>
<h3 id="adaboost算法">8.1.2 AdaBoost算法</h3>
<p>本节讲述了算法及其说明：</p>
<ul>
<li>最终分类器是基本分类器<span class="math inline">\(G_m(x)\)</span>的加权求和，其中第m个基本分类器<span class="math inline">\(G_m(x)\)</span>的系数是<span class="math inline">\(\alpha_m = \frac{1-e_m}{e_m}\)</span>，其中<span class="math inline">\(e_m\)</span>是第<span class="math inline">\(m\)</span>次迭代时<span class="math inline">\(G_m(x)\)</span>分类错误的样本的权值之和。</li>
<li>权值更新是对上次迭代的权值加一个系数然后归一化，分类正确的样本的权重所加的系数是<span class="math inline">\(e^{-\alpha_m}\)</span>被缩小，分类错误的样本的权重所加系数是<span class="math inline">\(e^{\alpha_m}\)</span>被放大。</li>
</ul>
<h3 id="adaboost算法的分类误差分析">8.2 AdaBoost算法的分类误差分析</h3>
<ul>
<li>AdaBoost算法的最终分类器的训练误差是有上界的，该上界是各个分类器的权值分布的规范化因子(公式8.5)之积。</li>
<li>AdaBoost的训练误差是以指数速率下降的。</li>
</ul>
<h2 id="adaboost算法的解释">8.3 AdaBoost算法的解释</h2>
<ul>
<li>加法模型</li>
</ul>
<p><span class="math inline">\(f(x)=\sum_{m=1}^{M}\beta_mb(x;\gamma_m)\)</span>，其中<span class="math inline">\(\beta_mb(x;\gamma_m)\)</span>为基函数，<span class="math inline">\(\gamma_m\)</span>为基函数的参数，<span class="math inline">\(\beta_m\)</span>为基函数的系数。 + 损失函数为指数函数 <span class="math display">\[L(y,f(x))=exp(-yf(x))\]</span> + 学习算法为前向分布算法</p>
<p>因为学习的是加法模型(公式8.13)，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，那么可以简化优化的复杂度。</p>
<h2 id="提升树">8.4 提升树</h2>
<ul>
<li>本节的提升树是以回归树为基本分类器，与CART的回归树本质上没区别。。</li>
<li>AdaBoost的损失函数是指数函数，而提升树考虑了其它类型的损失函数。不同的损失函数，在每次找弱分类器所用的数据（包括权重）的产生方法不同
<ul>
<li>当损失函数是平方损失时，弱分类器的数据是上一轮所得到的模型与所用数据的差即残差。</li>
<li>对于更一般的损失函数，弱分类器的数据是损失函数的负梯度。损失函数时平方损失时的负梯度，就是残差。</li>
</ul></li>
</ul>
<h1 id="em算法及其推广">EM算法及其推广</h1>
<ul>
<li>EM算法主要应用于含有隐变量的概率模型的学习，如高斯混合模型和隐马尔可夫模型。
<ul>
<li>与隐变量对应的是观测变量，与变量对应的是参数。观测变量的数据又叫不完全数据，观测变量的数据与隐变量连在一起称为完全数据。要知道参数、隐变量和观测变量的区别。</li>
</ul></li>
<li>理论分析里E步和M步的工作: EM算法的目标函数是观测数据<span class="math inline">\(Y\)</span>关于参数<span class="math inline">\(\theta\)</span>的对数自然函数，即<span class="math inline">\(L(\theta)=\log P(Y\mid \theta)=\log \left(\sum_Z P(Y\mid Z,\theta)P(Z\mid \theta)\right)\)</span>
<ul>
<li>E步：</li>
</ul>
计算完全数据的对数似然函数<span class="math inline">\(\log P(Y,Z\mid \theta)\)</span>关于在给定观测数据<span class="math inline">\(Y\)</span>和当前参数<span class="math inline">\(\theta^{(i)}\)</span>下对隐变量<span class="math inline">\(Z\)</span>的条件概率分布<span class="math inline">\(P(Z\mid Y,\theta^{(i)})\)</span>的期望，即EM算法里的Q函数： <span class="math display">\[Q(\theta,\theta^{(i)})=\sum_{Z}\log P(Y,Z\mid \theta)P(Z\mid Y,\theta^{(i)})\]</span>
<ul>
<li>M步：</li>
</ul>
<p>求<span class="math inline">\(\theta\)</span>使得最大化Q函数</p></li>
<li>编程实践里E步和M步的工作
<ul>
<li>E步：在当前模型参数之下结合观测变量集，计算观测变量每个取值分别来自于每个隐变量取值的概率</li>
</ul>
GMM模型里是是[0,1]区间的变量（p164 <span class="math inline">\(\hat{\gamma_{jk}}\)</span>）；而K均值里是0-1变量；而例9.1里隐变量是观测变量来自于硬币B或C的概率。
<ul>
<li>M步：计算模型参数</li>
</ul>
<p>GMM模型里计算模型参数见公式9.30-9.32，其模型参数分两部分，即<span class="math inline">\(K\)</span>个高斯模型的参数<span class="math inline">\((\mu_k,\sigma_k^2)\)</span>及其权重<span class="math inline">\(\alpha_k\)</span>；而K均值里，参数是各个聚类中心，注意其没有权重的概念；而例9.1里参数是(<span class="math inline">\(\pi,p,q\)</span>)</p></li>
</ul>
<h1 id="第10章-隐马尔科夫链">第10章 隐马尔科夫链</h1>
<h2 id="隐马尔可夫模型的基本概念">10.1 隐马尔可夫模型的基本概念</h2>
<h3 id="隐马模型的定义">10.1.1 隐马模型的定义</h3>
<ul>
<li>状态序列和观测序列等长，元素一一对应</li>
<li>隐马模型<span class="math inline">\(\lambda\)</span>的三元组表示法<span class="math inline">\(\lambda=(A,B,\pi)\)</span>。</li>
</ul>
<p>其中初始状态概率向量<span class="math inline">\(\pi\)</span>和状态转移矩阵<span class="math inline">\(A\)</span>确定了隐藏的马尔科夫链（即不可观测到的状态序列<span class="math inline">\(I\)</span>），而观测概率矩阵<span class="math inline">\(B\)</span>确定了如何从状态生成观测，与状态序列<span class="math inline">\(I\)</span>综合确定了如何产生观测序列<span class="math inline">\(O\)</span>。</p>
<p>用于标注问题时，状态对应着要求给出的标记，观测序列对应要被标注的数据。</p>
<ul>
<li>隐马模型的两个基本假设
<ul>
<li>齐次性假设</li>
<li>观测独立性假设 t ### 10.1.3 隐马模型的三个基本问题</li>
</ul></li>
</ul>
<p>可以用词性标注问题来理解这三类问题。</p>
<ul>
<li>概率计算问题</li>
</ul>
<p>给定<span class="math inline">\(\lambda\)</span>和<span class="math inline">\(O\)</span>，计算<span class="math inline">\(P(O\mid \lambda)\)</span>；词性标注场景里，给定模型参数，计算词序列概率。</p>
<ul>
<li>学习问题
<ul>
<li><p>有<span class="math inline">\(I\)</span>，监督学习：已知<span class="math inline">\(O\)</span>和<span class="math inline">\(I\)</span>,推断<span class="math inline">\(\lambda\)</span>，使得<span class="math inline">\(P(O\mid \lambda)\)</span>最大</p></li>
<li><p>无<span class="math inline">\(I\)</span>，属于无监督学习：已知<span class="math inline">\(O\)</span>,推断<span class="math inline">\(\lambda\)</span>，使得<span class="math inline">\(P(O\mid \lambda)\)</span>最大；词性标注场景里，给定词序列，去学习模型参数。</p></li>
</ul></li>
<li>预测问题</li>
</ul>
<p>已知<span class="math inline">\(\lambda\)</span>和<span class="math inline">\(O\)</span>，给出<span class="math inline">\(I = argmax P(I\mid O)\)</span>；词性标注场景里，给定模型和分好的词，来标注词性。</p>
<h2 id="概率计算算法">10.2 概率计算算法</h2>
<h3 id="直接计算法">10.2.1 直接计算法</h3>
<p><span class="math inline">\(P(O\mid \lambda)=\sum_{I}P(O\mid I,\lambda)P(I\mid \lambda)=\sum_{i}\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)\cdots a_{i_{T-1}i_T}b_{i_T}(o_T)\)</span> 计算量很大，是O(TN^T)阶的。</p>
<h3 id="前向算法和后向算法">10.2.2-10.2.3 前向算法和后向算法</h3>
<ul>
<li>前向算法和后向算法都属于动态规划</li>
<li>两个概念是理解算法的钥匙
<ul>
<li>前向概率：给定隐马模型<span class="math inline">\(\lambda\)</span>，定义到时刻<span class="math inline">\(t\)</span>部分观测序列为<span class="math inline">\(o_1,o_2,\cdots,o_t\)</span>且状态为<span class="math inline">\(q_i\)</span>的概率为前向概率，记作<span class="math display">\[\alpha_t(i)=P(o_1,o_2,\cdots,o_t,i_t=q_i\mid \lambda)\]</span> 递推公式有，<span class="math inline">\(\alpha_1(i)=\pi_ib_i(o_1)\)</span>,而对于<span class="math inline">\(t=1,2,\cdots,T-1\)</span> <span class="math display">\[\alpha_{t+1}(i)=\left(\sum_{j=1}^{N}\alpha_t(j)a_{ji}\right)b_i(o_{t+1})\]</span> 终止时有 <span class="math display">\[P(O\mid \lambda)=\sum_{i=1}^{N}\alpha_T(i)\]</span></li>
<li>后向概率：给定隐马模型<span class="math inline">\(\lambda\)</span>，定义到时刻<span class="math inline">\(t\)</span>状态为<span class="math inline">\(q_i\)</span>的条件下，从<span class="math inline">\(t+1\)</span>到<span class="math inline">\(T\)</span>的部分观测序列为<span class="math inline">\(o_{t+1},o_{t+2},\cdots,o_T\)</span>的概率为后向概率，记作<span class="math display">\[\beta_t(i)=P(o_{t+1},o_{t+2},\cdots,o_T\mid i_t=q_i,\lambda)\]</span> 递推公式有，<span class="math inline">\(\beta_T(i)=1\)</span>，而对于<span class="math inline">\(t=T-1,T-2,\cdots,1\)</span> <span class="math display">\[\beta_t(i)=\sum_{j=1}^{N}a_{ij}b_j(o_{t+1})\beta_{t+1}(j)\]</span> 终止时有<span class="math display">\[P(O\mid \lambda)=\sum_{i=1}^{N}\pi_ib_i(o_1)\beta_1(i)\]</span></li>
</ul></li>
</ul>
<h2 id="学习算法">10.3 学习算法</h2>
<h3 id="监督学习方法">10.3.1 监督学习方法</h3>
<p>用极大似然估计法，可以通过直接统计得到<span class="math inline">\(\lambda\)</span>各参数。</p>
<h3 id="baum-welch算法">10.3.2 Baum-Welch算法</h3>
<p>属于无监督学习，期望最大化方法：把观测序列看做观测数据，将状态序列看做隐数据，那么隐马模型是一个含有隐变量的概率模型<span class="math inline">\(P(O\mid \lambda)=\sum_IP(O\mid I,\lambda)P(I\mid \lambda)\)</span> + EM算法的E步的Q函数是<span class="math inline">\(Q(\lambda,\hat{\lambda})=\sum_I\log P(O,I\mid \lambda)P(O,I\mid \hat{\lambda})\)</span>，其中之所以可以采用<span class="math inline">\(P(O,I\mid \hat{\lambda})\)</span>而非EM常用的<span class="math inline">\(P(I\mid O,\hat{\lambda})\)</span>是因为二者有常数比例的关系：<span class="math inline">\(P(\hat{\lambda})=\frac{P(O,I\mid \hat{\lambda})}{P(I\mid O,\hat{\lambda})}\)</span>。 + Baum-Welch算法的E步只列出了Q函数，并没有显式地计算期望；然后直接在M步,利用概率的性质，通过拉格朗日算子法来计算参数。</p>
<h2 id="预测算法">10.4 预测算法</h2>
<h3 id="维特比算法">10.4.2 维特比算法</h3>
<ul>
<li>维特比算法，和前向算法和后向算法一样，都属于动态规划</li>
</ul>
<h1 id="第11章-条件随机场">第11章 条件随机场</h1>
<h2 id="概率无向图模型">11.1 概率无向图模型</h2>
<p>三个概念：</p>
<ul>
<li>成对、局部、全局马尔科夫性，三者定义是等价的</li>
<li>概率无向图模型基于马尔科夫性的定义:联合概率分布<span class="math inline">\(P(Y)\)</span>满足上述马尔科夫性。</li>
<li>概率无向图的最大团<span class="math inline">\(C\)</span>的概念，即基于最大团的因式分解</li>
</ul>
<p>概率无向图模型的联合概率分布<span class="math inline">\(P(Y)\)</span>可以表示为如下形式： <span class="math display">\[P(Y)=\frac{1}{Z}\prod_C\Psi_C(Y_C)\]</span> 其中<span class="math inline">\(Z=\sum_Y\prod_C\Psi_C(Y_C)\)</span>，而<span class="math inline">\(\Psi_C(Y_C)\)</span>称为势函数，通常定义为指数函数<span class="math inline">\(\Psi_C(Y_C)=exp\left(-E(Y_C)\right)\)</span>。</p>
<h2 id="条件随机场的定义与形式">11.2 条件随机场的定义与形式</h2>
<ul>
<li>线性链条件随机场的参数化形式 <span class="math display">\[P(y\mid x)=\frac{1}{Z(x)}exp\left(\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_ls_l(y_i,x,i)\right)\]</span> 其中<span class="math inline">\(Z(x)=\sum_{y}exp\left(\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_ls_l(y_i,x,i)\right)\)</span>，其中<span class="math inline">\(t_k(y_{i-1},y_i,x,i)\)</span>是转移函数，<span class="math inline">\(s_l(y_i,x,i)\)</span>是状态函数。</li>
</ul>
<h2 id="条件随机场的概率计算问题">11.3 条件随机场的概率计算问题</h2>
<p>前向后向算法</p>
<h2 id="条件随机场的学习算法">11.4 条件随机场的学习算法</h2>
<p>学习方法是极大似然估计或者正则化的极大似然估计，优化算法是牛顿、拟牛顿法</p>
<h2 id="条件随机场的预测算法">11.5 条件随机场的预测算法</h2>
<p>维特比算法</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag">#机器学习</a>
          
          <a href="https://github.com/wolfbrother/wolfbrother.github.io/blob/hexo/source/_posts/ML-AI/2019-09-09-统计学习方法的总结.md" rel="tag" target="_blank">&#64gitRepo</a>
        </div>
      

      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/09/09/algs/general/2019-09-10-排序算法/" rel="next" title="排序算法">
                <i class="fa fa-chevron-left"></i> 排序算法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/09/09/ML-AI/2019-09-09-机器学习概念总结三分之一部/" rel="prev" title="机器学习概念总结三分之一部">
                机器学习概念总结三分之一部 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="wolfbrother">
            
              <p class="site-author-name" itemprop="name">wolfbrother</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">88</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/wolfbrother/wolfbrother.github.io/issues" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:jlxufly@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#第二章感知机"><span class="nav-number">1.</span> <span class="nav-text">第二章感知机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#感知机学习算法的原始形式"><span class="nav-number">1.0.1.</span> <span class="nav-text">2.3.1 感知机学习算法的原始形式</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#第三章-k近邻法"><span class="nav-number">2.</span> <span class="nav-text">第三章 k近邻法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#k值的选择"><span class="nav-number">2.0.1.</span> <span class="nav-text">3.2.3 k值的选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分类决策规则"><span class="nav-number">2.0.2.</span> <span class="nav-text">3.2.4 分类决策规则</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第四章-朴素贝叶斯法"><span class="nav-number">3.</span> <span class="nav-text">第四章 朴素贝叶斯法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#朴素贝叶斯的学习与分类"><span class="nav-number">3.1.</span> <span class="nav-text">4.1 朴素贝叶斯的学习与分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#后验概率最大化的含义"><span class="nav-number">3.1.1.</span> <span class="nav-text">4.1.2 后验概率最大化的含义</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#朴素贝叶斯的参数估计"><span class="nav-number">3.2.</span> <span class="nav-text">4.2 朴素贝叶斯的参数估计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#贝叶斯估计"><span class="nav-number">3.2.1.</span> <span class="nav-text">4.2.3 贝叶斯估计</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第五章-决策树"><span class="nav-number">4.</span> <span class="nav-text">第五章 决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树模型与学习"><span class="nav-number">4.1.</span> <span class="nav-text">5.1 决策树模型与学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于信息增益的特征选择"><span class="nav-number">4.2.</span> <span class="nav-text">5.2 基于信息增益的特征选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树的生成"><span class="nav-number">4.3.</span> <span class="nav-text">5.3 决策树的生成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树的剪枝"><span class="nav-number">4.4.</span> <span class="nav-text">5.4 决策树的剪枝</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cart算法"><span class="nav-number">4.5.</span> <span class="nav-text">5.5 CART算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cart生成"><span class="nav-number">4.5.1.</span> <span class="nav-text">5.5.1 CART生成</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第六章-逻辑斯蒂回归于最大熵模型"><span class="nav-number">5.</span> <span class="nav-text">第六章 逻辑斯蒂回归于最大熵模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#逻辑斯蒂回归模型"><span class="nav-number">5.1.</span> <span class="nav-text">逻辑斯蒂回归模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最大熵模型"><span class="nav-number">5.2.</span> <span class="nav-text">6.2 最大熵模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#最大熵原理"><span class="nav-number">5.2.1.</span> <span class="nav-text">6.2.1 最大熵原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#最大熵模型的学习"><span class="nav-number">5.2.2.</span> <span class="nav-text">6.2.3 最大熵模型的学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#极大似然估计"><span class="nav-number">5.2.3.</span> <span class="nav-text">6.2.4 极大似然估计</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第七章-支持向量机"><span class="nav-number">6.</span> <span class="nav-text">第七章 支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#线性可分支持向量机与硬间隔最大化"><span class="nav-number">6.1.</span> <span class="nav-text">7.1 线性可分支持向量机与硬间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#学习的对偶算法"><span class="nav-number">6.1.1.</span> <span class="nav-text">7.1.4 学习的对偶算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性支持向量机与软间隔最大化"><span class="nav-number">6.2.</span> <span class="nav-text">7.2 线性支持向量机与软间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#支持向量"><span class="nav-number">6.2.1.</span> <span class="nav-text">7.2.3 支持向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#合页损失函数"><span class="nav-number">6.2.2.</span> <span class="nav-text">合页损失函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#非线性支持向量机与核函数"><span class="nav-number">6.3.</span> <span class="nav-text">7.3 非线性支持向量机与核函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#序列最小最优化算法smo"><span class="nav-number">6.4.</span> <span class="nav-text">7.4 序列最小最优化算法SMO</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第八章-提升方法"><span class="nav-number">7.</span> <span class="nav-text">第八章 提升方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#提升方法的基本思路"><span class="nav-number">7.0.1.</span> <span class="nav-text">8.1.1 提升方法的基本思路</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adaboost算法"><span class="nav-number">7.0.2.</span> <span class="nav-text">8.1.2 AdaBoost算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adaboost算法的分类误差分析"><span class="nav-number">7.0.3.</span> <span class="nav-text">8.2 AdaBoost算法的分类误差分析</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adaboost算法的解释"><span class="nav-number">7.1.</span> <span class="nav-text">8.3 AdaBoost算法的解释</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#提升树"><span class="nav-number">7.2.</span> <span class="nav-text">8.4 提升树</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#em算法及其推广"><span class="nav-number">8.</span> <span class="nav-text">EM算法及其推广</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第10章-隐马尔科夫链"><span class="nav-number">9.</span> <span class="nav-text">第10章 隐马尔科夫链</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#隐马尔可夫模型的基本概念"><span class="nav-number">9.1.</span> <span class="nav-text">10.1 隐马尔可夫模型的基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#隐马模型的定义"><span class="nav-number">9.1.1.</span> <span class="nav-text">10.1.1 隐马模型的定义</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#概率计算算法"><span class="nav-number">9.2.</span> <span class="nav-text">10.2 概率计算算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#直接计算法"><span class="nav-number">9.2.1.</span> <span class="nav-text">10.2.1 直接计算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前向算法和后向算法"><span class="nav-number">9.2.2.</span> <span class="nav-text">10.2.2-10.2.3 前向算法和后向算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习算法"><span class="nav-number">9.3.</span> <span class="nav-text">10.3 学习算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#监督学习方法"><span class="nav-number">9.3.1.</span> <span class="nav-text">10.3.1 监督学习方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#baum-welch算法"><span class="nav-number">9.3.2.</span> <span class="nav-text">10.3.2 Baum-Welch算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#预测算法"><span class="nav-number">9.4.</span> <span class="nav-text">10.4 预测算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#维特比算法"><span class="nav-number">9.4.1.</span> <span class="nav-text">10.4.2 维特比算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第11章-条件随机场"><span class="nav-number">10.</span> <span class="nav-text">第11章 条件随机场</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#概率无向图模型"><span class="nav-number">10.1.</span> <span class="nav-text">11.1 概率无向图模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#条件随机场的定义与形式"><span class="nav-number">10.2.</span> <span class="nav-text">11.2 条件随机场的定义与形式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#条件随机场的概率计算问题"><span class="nav-number">10.3.</span> <span class="nav-text">11.3 条件随机场的概率计算问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#条件随机场的学习算法"><span class="nav-number">10.4.</span> <span class="nav-text">11.4 条件随机场的学习算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#条件随机场的预测算法"><span class="nav-number">10.5.</span> <span class="nav-text">11.5 条件随机场的预测算法</span></a></li></ol></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">狼哥空间</span>

  
</div>












        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: true,
        appId: 'iKewncyt21DH3cd1bESqHH10-gzGzoHsz',
        appKey: 'EpCP2TXQXUIm4jMMbzLk3r0u',
        placeholder: '(>﹏<) Just show your thinking~',
        avatar:'monsterid',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
