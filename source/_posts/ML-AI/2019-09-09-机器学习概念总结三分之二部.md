---
dtindex: 2019-09-09机器学习概念
title: 机器学习概念总结三分之二部
categories: ML-AI
tags: 机器学习
author: wolfbrother
date: 2019-09-09 
---

## 过拟合

随着训练过程的进行，在训练集上的错误率渐渐减小，但是在验证集上的错误率却反而渐渐增大。也就是说训练出来的网络过拟合了训练集，对训练集外的数据却不工作。

过拟合是一种在项目实践中遇到的一个常见的现象，并不是一种高深的理论。“简单函数的泛化能力更好”不是一个有着坚实理论和数学基础的理论定理，它只是在长久的数据科学项目中，数据科学家们发现的一个普遍现象。可以理解为属于 经验科学 的一个范畴，简单的模型不容易产生过拟合，简单的模型泛化能力更好，甚至所谓的奥卡姆剃刀原理。这个经验在很多时候是有效的，我们也没有什幺理由不去应用这个经验。毕竟数据科学还是一个偏向实践和以结果说话的学科，得到好的结果是最重要的。

过拟合的原因 （[大白话解释模型产生过拟合的原因](https://zhuanlan.zhihu.com/p/26122044  )有具体的例子，[浅议过拟合现象(overfitting)以及正则化技术原理](https://flashgene.com/archives/11824.html )）：
+ 数据有噪声
+ 训练数据不足，在有限的样本中搜索过大的模型空间
+ 训练模型过度导致模型非常复杂


避免过拟合的方法 （[大白话解释模型产生过拟合的原因](https://zhuanlan.zhihu.com/p/26122044 )）：

+ early stopping
+ 数据集扩增（Data augmentation）
+ 正则化（Regularization）包括L1、L2（L2 regularization也叫weight decay）
+ dropout ([正则化为什么能防止过拟合](https://www.cnblogs.com/alexanderkun/p/6922428.html ))。
+ 最大池化层（maxpool）

其它：

+ [正则化为什么能防止过拟合](https://www.jianshu.com/p/a9cc62db419e) 直观地说明了在多项式拟合里，为什么过拟合的系数比较大，进而说明正则化（不管是L1还是L2都能让参数趋于小）对过拟合有用：过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。
+ 术语"泛化"指的是一个假设模型能够应用到新样本的能力。新样本数据是指没有出现在训练集中的数据。

#### [高偏差与高方差](https://www.jianshu.com/p/a585d5506b1e )

+ 过拟合意味着高方差，欠拟合意味着高偏差（https://www.cnblogs.com/jianxinzhou/p/4083921.html ）
+ 偏差（bias）：我们要预测第二天的收益，而估值指标经常用于长线投资，虽然每次预测都是信誓旦旦，但是模型从本质上就把目标搞错了。
+ 方差（variance）：过多的已知条件，导致模型无法给出确定的预测，预测结果和瞎蒙一样，给人的感觉是不靠谱。
+ 无法消除的误差：我们都知道，完美预测第二天的情况，是不可能的。这样的误差难以消除，我们希望它越小越好，一般就忽略掉了。所以，我们可以优化的误差=偏差+方差
相关描述可以看一下知乎答案：[链接](https://www.zhihu.com/question/20700829/answer/119314862 )




## 贝叶斯学派和频率学派
+ 先验概率、似然函数与后验概率

似然函数（likelihood function），也称作似然，是一个关于统计模型参数的函数。也就是这个函数中自变量是统计模型的参数。对于结果 x ，在参数集合 θ 上的似然，就是在给定这些参数值的基础上，观察到的结果的概率 $L(θ\mid x)=P(x\mid θ)$。也就是说，似然是关于参数的函数，在参数给定的条件下，对于观察到的 x 的值的条件分布。

用$(θ)$表示概率分布函数，用 $p(x\mid θ)$表示观测值 x 的似然函数。后验概率定义为：
$p(θ\mid x)=\frac{p(x\mid θ)p(θ)}{p(x)}$ 。由于分母不变，可以表达成如下正比关系：
后验概率$p(θ\mid x)$ ∝ 似然函数$p(x\mid θ)$×先验概率p(x)

[先验概率、似然函数与后验概率](https://www.cnblogs.com/wjgaas/p/4523779.html ) 最大后验概率和极大似然估计很像，只是多了一项先验分布，它体现了贝叶斯认为参数也是随机变量的观点，在实际运算中通常通过超参数给出先验分布。极大似然估计和最大后验概率都是参数的点估计。在频率学派中，参数固定了，预测值也就固定了。最大后验概率是贝叶斯学派的一种近似手段，因为完全贝叶斯估计不一定可行。另一方面，最大后验概率可以看作是对先验和MLE的一种折衷，如果数据量足够大，最大后验概率和最大似然估计趋向于一致，如果数据为0,最大后验仅由先验决定。

用抛硬币的例子来依次讲解极大似然估计、最大后验概率估计和贝叶斯估计：[链接]( https://blog.csdn.net/yangliuy/article/details/8296481 )

## 拉格朗日乘子法

分为只含有等式约束，以及含有不等式约束两种情况。

####  只有等式约束

参考 维基百科[拉格朗日乘数](https://zh.wikipedia.org/wiki/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0 )。例如如下特征，即目标函数取极值时目标函数和各个等式约束的在该极值处相切，即它们在极值处梯度方向相等或者相反，即它们的各个偏导数成比例关系。

举例：$f(x,y)$在$g(x,y) = c$约束下的极值问题，则极值点$(x_0,y_0)$处有$f_x+\lambda g_x = 0, f_y+\lambda g_y=0$，同时有$g(x_0,y_0)=c$。因此就将原来的约束优化问题，转换成如下拥有三个自变量$(x,y,\lambda)$的无约束极值问题 $f(x,y)+\lambda (g(x,y)-c)$，其中$\lambda$任意取值。

对该无约束问题进行分析，由于$\lambda$可以任意取值，因此只能由$g(x,y)-c=0$，另外取极值时梯度为0，即$f'+\lambda g'=0$，可见于原约束优化问题是等价的。

其中原约束优化问题是“原始问题”，转换后的无约束优化问题是“对偶问题”，二者最优解等价的条件就是取该最优解时满足$f'+\lambda g'=0$和$g=c$。

#### 含有不等式约束
参考《统计学习方法》附录C的“[拉格朗日对偶性](https://zhuanlan.zhihu.com/p/38182879 )”，关于原始问题和对偶问题的最优解相等的时候该最优解必须满足的KKT条件。

参考等式约束情况下原始问题到对偶问题的转化，以及最优解需满足的KKT条件（包含两部分，原始问题的约束条件，以及梯度条件）。

#### 对偶问题和KKT条件

猜测引入对偶问题的原因，是方便推导出KKT条件，进而通过求解KKT条件来求解原始问题。

## 共轭先验

在贝叶斯统计中，如果后验分布与先验分布属于同类，则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验。共轭先验的好处主要在于代数上的方便性，可以直接给出后验分布的封闭形式，否则的话只能数值计算。共轭先验也有助于获得关于似然函数如何更新先验分布的直观印象。


